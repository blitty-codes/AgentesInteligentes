{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Librerías y setup"
      ],
      "metadata": {
        "id": "o6jC-C90SNfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-model-optimization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pHEqtWySLpj",
        "outputId": "928302ed-68fa-45d2-c211-08e3a3f09e0c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sdmTuAa4Rz1e"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivos de la compresión\n",
        "\n",
        "La calidad de un algoritmo de compresión se evaluará en base a las siguientes reglas:\n",
        "\n",
        "1. El nivel de compresión (en parámetros o memoria) debe maximizarse y la pérdida de precisión debe minimizarse\n",
        "2. Debe proporcionar el máximo nivel de compresión en memoria\n",
        "3. Debe maximizar la velocidad de inferencia\n"
      ],
      "metadata": {
        "id": "WVVy785xoaYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento de un modelo sin reducir (MNIST)"
      ],
      "metadata": {
        "id": "InFy1ZrhSQMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define the model architecture.\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=6,\n",
        "  validation_split=0.1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZp-vE02R3xb",
        "outputId": "a340dd31-50f2-42f0-d556-e8ea3665c6b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/6\n",
            "1688/1688 [==============================] - 20s 11ms/step - loss: 0.3113 - accuracy: 0.9108 - val_loss: 0.1279 - val_accuracy: 0.9668\n",
            "Epoch 2/6\n",
            "1688/1688 [==============================] - 14s 8ms/step - loss: 0.1272 - accuracy: 0.9644 - val_loss: 0.0876 - val_accuracy: 0.9767\n",
            "Epoch 3/6\n",
            "1688/1688 [==============================] - 14s 8ms/step - loss: 0.0916 - accuracy: 0.9738 - val_loss: 0.0772 - val_accuracy: 0.9792\n",
            "Epoch 4/6\n",
            "1688/1688 [==============================] - 15s 9ms/step - loss: 0.0764 - accuracy: 0.9779 - val_loss: 0.0711 - val_accuracy: 0.9792\n",
            "Epoch 5/6\n",
            "1688/1688 [==============================] - 15s 9ms/step - loss: 0.0667 - accuracy: 0.9805 - val_loss: 0.0635 - val_accuracy: 0.9808\n",
            "Epoch 6/6\n",
            "1688/1688 [==============================] - 14s 9ms/step - loss: 0.0609 - accuracy: 0.9817 - val_loss: 0.0728 - val_accuracy: 0.9783\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d36c3fc40>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluamos la precisión base y guardamos el modelo para medir su peso en memoria"
      ],
      "metadata": {
        "id": "qijI-D46SzHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_total = 0\n",
        "\n",
        "for i in range(10):\n",
        "  t_start = time.perf_counter()\n",
        "  _, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "  t_end = time.perf_counter()\n",
        "\n",
        "  t_total = t_total + (t_end - t_start)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "\n",
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
        "print('Saved baseline model to:', keras_file)\n",
        "\n",
        "print('Average inference time (seconds): ',  t_total/10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUpuhPVQSEJr",
        "outputId": "dbcf4f3a-a20d-4fe0-a402-b090a17510b1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy: 0.9700999855995178\n",
            "Saved baseline model to: /tmp/tmpdjvyzzme.h5\n",
            "Average inference time (seconds):  1.3560929621000468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo base realiza su inferencia en 1.35 segundos."
      ],
      "metadata": {
        "id": "XXwYNnCtf-06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Poda"
      ],
      "metadata": {
        "id": "KqCKw0ePXu-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podamos el modelo pre-entrenado por medio de `sparsity`, que sustituye los pesos menos significativos por 0.\n",
        "\n"
      ],
      "metadata": {
        "id": "yc0VmQe6UFNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define model for pruning.\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mdoC-4yUId6",
        "outputId": "be1f7441-2bd8-4144-baed-dbe156202b5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_reshape  (None, 28, 28, 1)        1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d   (None, 26, 26, 12)       230       \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_max_poo  (None, 13, 13, 12)       1         \n",
            " ling2d (PruneLowMagnitude)                                      \n",
            "                                                                 \n",
            " prune_low_magnitude_flatten  (None, 2028)             1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_dense (  (None, 10)               40572     \n",
            " PruneLowMagnitude)                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40,805\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 20,395\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "model_for_pruning.fit(train_images, train_labels,\n",
        "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
        "                  callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI0p_VRgUJXU",
        "outputId": "fa19f1d6-4062-4b89-aa4b-72af26cab77a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "422/422 [==============================] - 15s 29ms/step - loss: 0.0907 - accuracy: 0.9741 - val_loss: 0.1161 - val_accuracy: 0.9703\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 10s 23ms/step - loss: 0.1128 - accuracy: 0.9696 - val_loss: 0.0917 - val_accuracy: 0.9762\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d1c0fbaf0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_pruning.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yA7kG-HnQiR",
        "outputId": "8524b50d-e552-4ce8-cedb-c748e5f19c25"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_reshape  (None, 28, 28, 1)        1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d   (None, 26, 26, 12)       230       \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_max_poo  (None, 13, 13, 12)       1         \n",
            " ling2d (PruneLowMagnitude)                                      \n",
            "                                                                 \n",
            " prune_low_magnitude_flatten  (None, 2028)             1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_dense (  (None, 10)               40572     \n",
            " PruneLowMagnitude)                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40,805\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 20,395\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El algoritmo introduce varios pesos adicionales para calcular los menos significativos, despojamos estos pesos extra y guardamos el modelo."
      ],
      "metadata": {
        "id": "k1-AOEBCUomt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN2eoPg7UrfV",
        "outputId": "6f4aec5c-089e-417a-a46f-e6513cabbe10"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned Keras model to: /tmp/tmp6fepv87k.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_export.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcowQCpjnb3D",
        "outputId": "321f83d8-731e-4a4e-89f3-b0aa3283ce03"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 12)        120       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 12)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2028)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                20290     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,410\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que ya no hay pesos adicionales."
      ],
      "metadata": {
        "id": "xeN2L6PrneJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_export.compile()\n",
        "\n",
        "t_total = 0\n",
        "\n",
        "for i in range(10):\n",
        "  t_start = time.perf_counter()\n",
        "  _, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "   test_images, test_labels, verbose=0)\n",
        "  t_end = time.perf_counter()\n",
        "\n",
        "  t_total = t_total + (t_end - t_start)\n",
        "\n",
        "print(\"Average inference time (seconds): \", t_total/10)\n",
        "print('Baseline test accuracy:', baseline_model_accuracy) \n",
        "print('Pruned test accuracy:', model_for_pruning_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlYkuhJm04V5",
        "outputId": "7b6d6cb3-419f-41ec-87ca-1fbd5cbe6c00"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average inference time (seconds):  1.195314035599972\n",
            "Baseline test accuracy: 0.9700999855995178\n",
            "Pruned test accuracy: 0.9700999855995178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El tiempo de inferencia se reduce a 1.19 segundos, con la precisión inalterada."
      ],
      "metadata": {
        "id": "lwmCXCIDgCnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora definiremos funciones para medir el tamaño en memoria de los modelos, comprimidos en memoria con gzip y sin comprimir. La compresión en memoria puede ser beneficiosa si se sustituyen muchos valores por 0, ya que los algoritmos de compresión como gzip tendrán facilidad para agrupar los valores establecidos a 0:"
      ],
      "metadata": {
        "id": "f5Noa78QVmRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "stE51Oq7UrvW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normal_model_size(file):\n",
        "  return os.path.getsize(file)"
      ],
      "metadata": {
        "id": "JlE2ht5eYd8X"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "# print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zer_4FU5VoTQ",
        "outputId": "4adbd870-41f0-4d54-9303-523754da5f0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of gzipped baseline Keras model: 78127.00 bytes\n",
            "Size of gzipped pruned Keras model: 25736.00 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of baseline Keras model: %.2f bytes\" % (get_normal_model_size(keras_file)))\n",
        "print(\"Size of pruned Keras model: %.2f bytes\" % (get_normal_model_size(pruned_keras_file)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajzcq2huYmpv",
        "outputId": "322a0fd4-058c-4525-ad24-dea966615c48"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of baseline Keras model: 98968.00 bytes\n",
            "Size of pruned Keras model: 98968.00 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones\n",
        "\n",
        "1. La precisión se ha reducido en algo menos de 0.02%.\n",
        "\n",
        "2. El tamaño en memoria se ha reducido 3 veces al aplicar gzip, seguramente debido a que `sparsity` sustituye pesos por 0, permitiendo que el algoritmo de compresión en almacenamiento agrupe los valores a 0 de forma eficiente.\n",
        "\n",
        "3. La velocidad de inferencia se ha acelerado en aproximadamente 0.12 segundos.\n",
        "\n",
        "En conclusión, este método de poda por `sparsity` resulta efectivo (de cara a reducción en almacenamiento) cuando se combina con un algoritmo de compresión para el fichero de salida. "
      ],
      "metadata": {
        "id": "M4gYhnL5Evre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cuantización\n",
        "\n",
        "Este proceso convertirá los pesos en tipo int8 (entero de 8 bits) y las funciones de activación en uint8 (unsigned integer de 8 bits)."
      ],
      "metadata": {
        "id": "tomgZAq7X2ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for for quantization aware.\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeGNyJWKX4Kk",
        "outputId": "73ec94c1-e3ff-4c33-860a-0a8f80417a50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (None, 28, 28)           3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_reshape (QuantizeWrap  (None, 28, 28, 1)        1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrapp  (None, 26, 26, 12)       147       \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quanti  (None, 13, 13, 12)       1         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWrap  (None, 2028)             1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (None, 10)               20295     \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,448\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 38\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se introducen 38 parámetros adicionales para cálculos internos del algoritmo de cuantización."
      ],
      "metadata": {
        "id": "cIxfNJfIoEn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_subset = train_images[:1000] # out of 60000\n",
        "train_labels_subset = train_labels[:1000]\n",
        "\n",
        "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
        "                  batch_size=500, epochs=2, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXFa4PsnYBe8",
        "outputId": "598ea287-b165-4d2a-c111-d18437624255"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "2/2 [==============================] - 1s 145ms/step - loss: 0.0617 - accuracy: 0.9822 - val_loss: 0.0600 - val_accuracy: 0.9800\n",
            "Epoch 2/2\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 0.0593 - accuracy: 0.9833 - val_loss: 0.0604 - val_accuracy: 0.9800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d0dc7ad70>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, quantized_keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(q_aware_model, quantized_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', quantized_keras_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H8YUMoxuNWF",
        "outputId": "b63d63d3-4c52-4a62-e8f6-dd9332d7d1d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned Keras model to: /tmp/tmp8njuqvzr.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_total = 0\n",
        "\n",
        "for i in range(10):\n",
        "  t_start = time.perf_counter()\n",
        "  _, q_aware_model_accuracy = q_aware_model.evaluate(\n",
        "   test_images, test_labels, verbose=0)\n",
        "  t_end = time.perf_counter()\n",
        "\n",
        "  t_total = t_total + (t_end - t_start)\n",
        "\n",
        "\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Quant test accuracy:', q_aware_model_accuracy)\n",
        "print('Average inference time (seconds): ', t_total/10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xupf4_VlY1DQ",
        "outputId": "cbddb41d-17af-4495-c1bc-2f13b5ef2bbd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy: 0.9700999855995178\n",
            "Quant test accuracy: 0.9782000184059143\n",
            "Average inference time (seconds):  1.2131768901998838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El tiempo de inferencia se reduce, estando ahora en 1.21 segundos.\n",
        "Merece la pena observar que el nuevo modelo sobreespecializa, ya que presenta precisión ligeramente mayor."
      ],
      "metadata": {
        "id": "e2L1TCoCf08p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped quantized Keras model: %.2f bytes\" % (get_gzipped_model_size(quantized_keras_file)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEtfU6HTuWtv",
        "outputId": "f96bc1bd-7e67-487d-c995-18ad73a57a7f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of gzipped baseline Keras model: 25736.00 bytes\n",
            "Size of gzipped quantized Keras model: 56135.00 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of baseline Keras model: %.2f bytes\" % (get_normal_model_size(keras_file)))\n",
        "print(\"Size of pruned Keras model: %.2f bytes\" % (get_normal_model_size(quantized_keras_file)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R2HKlP0Z54a",
        "outputId": "1314c888-db52-4b88-8933-abccbd911b43"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of baseline Keras model: 98968.00 bytes\n",
            "Size of pruned Keras model: 116272.00 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curiosamente, sin gzip, el tamaño no disminuye, sino que aumenta ligeramente debido a los 38 parámetros que introduce el algoritmo de cuantización."
      ],
      "metadata": {
        "id": "LsskuaHVgTmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlnIki_5fRI8",
        "outputId": "6b9de406-0742-41af-da6f-4ea9875d2374"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, reshape_layer_call_fn, reshape_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones\n",
        "\n",
        "1. La precisión ha aumentado, conque el modelo puede estar sobreespecializando.\n",
        "\n",
        "2. El tamaño en memoria se ha reducido en un 50%.\n",
        "\n",
        "3. La velocidad de inferencia se acelera, aunque menos que en la poda.\n"
      ],
      "metadata": {
        "id": "OWnpEOJlZTcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation\n",
        "\n",
        "Vamos a crear un `distiller` personalizado, que tiene los siguientes componentes:\n",
        "\n",
        "- Un modelo profesor entrenado.\n",
        "- Un modelo alumno para entrenar.\n",
        "- Una función de pérdida del estudiante (diferencia entre las predicciones del estudiante y la verdad objetivo).\n",
        "- Una función de pérdida de destilación, junto con temperatura (diferencia entre las predicciones `soft` del estudiante y las etiquetas del profesor.\n",
        "- Un factor $\\alpha$ para ponderar las pérdidas del estudiante y la destilación.\n",
        "- Un optimizador para el estudiante.\n"
      ],
      "metadata": {
        "id": "nIXC2zgLgq1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super().__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=3,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "\n",
        "            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n",
        "            # The magnitudes of the gradients produced by the soft targets scale\n",
        "            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n",
        "            distillation_loss = (\n",
        "                self.distillation_loss_fn(\n",
        "                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "                )\n",
        "                * self.temperature**2\n",
        "            )\n",
        "\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results"
      ],
      "metadata": {
        "id": "lNtnEwbwgrGP"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En `train_step` se hace un paso hacia adelante del estudiante y el profesor.\n",
        "Calculamos `student loss`, la pérdida del estudiante y `distillation_loss`, pérdida de destilación con $\\alpha$ y $1-\\alpha$\n",
        "\n",
        "En `test_step` se evalúa el modelo estudiante."
      ],
      "metadata": {
        "id": "NOQgEVtegwke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No emplearemos las clases definidas anteriormente porque el estudiante debe tener kernels más pequeños que el profesor:"
      ],
      "metadata": {
        "id": "dBlLgUg0gy1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "0anI3KnxgshJ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparamos los modelos profesor y alumno:"
      ],
      "metadata": {
        "id": "zkCTB3WVg2fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# profesor\n",
        "alberto = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"teacher\",\n",
        ")\n",
        "\n",
        "# alumno(s)\n",
        "neuralhive = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"student\",\n",
        ")\n",
        "\n",
        "\n",
        "# Estudiante sin destilar para comparar más adelante\n",
        "student_scratch = keras.models.clone_model(neuralhive)"
      ],
      "metadata": {
        "id": "Tj5NnNNFg1C3"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del profesor con normalidad\n",
        "alberto.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Entrenamos y evaluamos al profesor con los datos\n",
        "alberto.fit(train_images[:1000], train_labels[:1000], epochs=5)\n",
        "alberto.evaluate(test_images[:1000], test_labels[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQhM-P3Ig5R3",
        "outputId": "76b92c93-a9a3-473a-b2f3-d7c13b664754"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "32/32 [==============================] - 9s 241ms/step - loss: 0.1793 - sparse_categorical_accuracy: 0.9430\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 9s 274ms/step - loss: 0.0858 - sparse_categorical_accuracy: 0.9750\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.0268 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 7s 224ms/step - loss: 0.0081 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 9s 267ms/step - loss: 0.0078 - sparse_categorical_accuracy: 0.9990\n",
            "32/32 [==============================] - 2s 67ms/step - loss: 0.2040 - sparse_categorical_accuracy: 0.9420\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.20396357774734497, 0.9419999718666077]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Destilamos el profesor al estudiante:"
      ],
      "metadata": {
        "id": "mJd6RUVvhLOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and compile distiller\n",
        "distiller = Distiller(student=neuralhive, teacher=alberto)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.1,\n",
        "    temperature=10,\n",
        ")\n",
        "\n",
        "# Distill teacher to student\n",
        "distiller.fit(train_images[:1000], train_labels[:1000], epochs=3)\n",
        "\n",
        "# Evaluate student on test dataset\n",
        "distiller.evaluate(test_images[:1000], test_labels[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s-8d8FjhMH2",
        "outputId": "38a336db-1cb4-4bdb-8d86-3fac87a35290"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "32/32 [==============================] - 3s 73ms/step - sparse_categorical_accuracy: 0.1310 - student_loss: 2.7248 - distillation_loss: 32.1509\n",
            "Epoch 2/3\n",
            "32/32 [==============================] - 3s 92ms/step - sparse_categorical_accuracy: 0.6070 - student_loss: 1.2488 - distillation_loss: 17.7150\n",
            "Epoch 3/3\n",
            "32/32 [==============================] - 2s 74ms/step - sparse_categorical_accuracy: 0.7980 - student_loss: 0.7616 - distillation_loss: 9.0952\n",
            "32/32 [==============================] - 0s 3ms/step - sparse_categorical_accuracy: 0.7830 - student_loss: 0.9040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7829999923706055, 1.4007198810577393]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el alumno con normalidad\n",
        "student_scratch.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate student trained from scratch.\n",
        "student_scratch.fit(train_images[:1000], train_labels[:1000], epochs=3)\n",
        "student_scratch.evaluate(test_images[:1000], test_labels[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsiQldfshxbO",
        "outputId": "c5ba27ab-6f1a-4b39-9566-1edc72fda3fb"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "32/32 [==============================] - 1s 7ms/step - loss: 1.8124 - sparse_categorical_accuracy: 0.5140\n",
            "Epoch 2/3\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7184 - sparse_categorical_accuracy: 0.7940\n",
            "Epoch 3/3\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4894 - sparse_categorical_accuracy: 0.8450\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5920 - sparse_categorical_accuracy: 0.8150\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5920273065567017, 0.8149999976158142]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(student_scratch.evaluate(test_images, test_labels))\n",
        "print(distiller.evaluate(test_images, test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9t4f9cOh8Y5",
        "outputId": "963d80ea-fa5c-41d5-910f-d9009a2491bb"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5363 - sparse_categorical_accuracy: 0.8371\n",
            "[0.5363089442253113, 0.8371000289916992]\n",
            "313/313 [==============================] - 1s 3ms/step - sparse_categorical_accuracy: 0.8110 - student_loss: 0.7725\n",
            "[0.8109999895095825, 0.6327903866767883]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos observar un ligero decremento de la precisión, un ligero aumento de loss, y una ligera disminución en el tiempo de inferencia."
      ],
      "metadata": {
        "id": "jCVwja35jD2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_total = 0\n",
        "\n",
        "for i in range(10):\n",
        "  t_start = time.perf_counter()\n",
        "  _, baseline_model_accuracy = distiller.evaluate(test_images, test_labels, verbose=0)\n",
        "  t_end = time.perf_counter()\n",
        "\n",
        "  t_total = t_total + (t_end - t_start)\n",
        "\n",
        "print(\"Average inference time (seconds): \", t_total/10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHpBICMgh8qa",
        "outputId": "6d460e60-2c6a-4d1b-f2a8-f7349b6446f7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average inference time (seconds):  1.1929158142000234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardemos los modelos para comparar su tamaño en memoria"
      ],
      "metadata": {
        "id": "rWc8AM8kj_FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, student_scratch_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(student_scratch, student_scratch_file, include_optimizer=False)\n",
        "print('Saved student from scratch model to:', student_scratch_file)\n",
        "\n",
        "\n",
        "_, student_final_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(neuralhive, student_final_file, include_optimizer=False)\n",
        "print('Saved final student model to:', student_final_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1j6AmqQj-sW",
        "outputId": "3f428deb-4f59-468e-8369-8b918cd1ed97"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved student from scratch model to: /tmp/tmplnjd0ht9.h5\n",
            "Saved final student model to: /tmp/tmpe2o6e941.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped quantized Keras model: %.2f bytes\" % (get_gzipped_model_size(student_final_file)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tu1OQ-MlVEh",
        "outputId": "f9d66841-3cca-457d-a644-cc0b4998286c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of gzipped baseline Keras model: 25736.00 bytes\n",
            "Size of gzipped quantized Keras model: 78472.00 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neuralhive.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_NoblqKlit4",
        "outputId": "45e5bb63-b696-4d59-f996-4d6c10170527"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"student\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 14, 14, 16)        160       \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 14, 14, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 14, 14, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 7, 7, 32)          4640      \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 1568)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                15690     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,490\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones\n",
        "\n",
        "1. La precisión disminuye en un 6%\n",
        "\n",
        "2. El tamaño en memoria aumenta.\n",
        "\n",
        "3. La velocidad de inferencia se acelera a 1.19 segundos, resultado similar a la poda.\n",
        "\n",
        "\n",
        "En conclusión, parece que Knowledge Distillation no tiene un gran impacto sobre el almacenamiento, pero acelera la inferencia de forma efectiva con escasa pérdida de precisión."
      ],
      "metadata": {
        "id": "y-2oADvmjc4T"
      }
    }
  ]
}