{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Competici贸n: _Rock, paper, scissors, lizard, Spock_<a id=\"top\"></a>\n",
    "    \n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliotecas usadas a lo largo del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "import itertools\n",
    "import pickle\n",
    "import typing\n",
    "import urllib\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definici贸n del juego"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elementos usados en el juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acciones posibles\n",
    "class Action(enum.Enum):\n",
    "    \"\"\"Cada una de las posibles figuras.\"\"\"\n",
    "    ROCK = ''\n",
    "    PAPER = 'Щ'\n",
    "    SCISSORS = '锔'\n",
    "    LIZARD = ''\n",
    "    SPOCK = ''\n",
    "\n",
    "# Recompensas asociadas al juego\n",
    "MOVES_AND_REWARDS = {\n",
    "    (Action.ROCK, Action.ROCK): 0, (Action.ROCK, Action.PAPER): -1,\n",
    "    (Action.ROCK, Action.SCISSORS): 1, (Action.ROCK, Action.LIZARD): 1,\n",
    "    (Action.ROCK, Action.SPOCK): -1,\n",
    "    (Action.PAPER, Action.ROCK): 1, (Action.PAPER, Action.PAPER): 0,\n",
    "    (Action.PAPER, Action.SCISSORS): -1, (Action.PAPER, Action.LIZARD): -1,\n",
    "    (Action.PAPER, Action.SPOCK): 1,\n",
    "    (Action.SCISSORS, Action.ROCK): -1, (Action.SCISSORS, Action.PAPER): 1,\n",
    "    (Action.SCISSORS, Action.SCISSORS): 0, (Action.SCISSORS, Action.LIZARD): 1,\n",
    "    (Action.SCISSORS, Action.SPOCK): -1,\n",
    "    (Action.LIZARD, Action.ROCK): -1, (Action.LIZARD, Action.PAPER): 1,\n",
    "    (Action.LIZARD, Action.SCISSORS): -1, (Action.LIZARD, Action.LIZARD): 0,\n",
    "    (Action.LIZARD, Action.SPOCK): 1,\n",
    "    (Action.SPOCK, Action.ROCK): 1, (Action.SPOCK, Action.PAPER): -1,\n",
    "    (Action.SPOCK, Action.SCISSORS): 1, (Action.SPOCK, Action.LIZARD): -1,\n",
    "    (Action.SPOCK, Action.SPOCK): 0,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase que define una partida entre dos jugadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    RENDER_MODE_HUMAN = 'human'\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def play(self, p1_action, p2_action):\n",
    "        result = MOVES_AND_REWARDS[(p1_action, p2_action)]\n",
    "        if self.render_mode == 'human':\n",
    "            self.render(p1_action, p2_action, result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def render(p1_action, p2_action, result):\n",
    "        if result == 0:\n",
    "            print(f'{p1_action.value} tie!')\n",
    "        elif result == 1:\n",
    "            print(f'{p1_action.value} beats {p2_action.value}')\n",
    "        elif result == -1:\n",
    "            print(f'{p2_action.value} beats {p1_action.value}')\n",
    "        else:\n",
    "            raise ValueError(f'{p1_action}, {p2_action}, {result}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definici贸n de comportamiento de un agente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transiciones que realizan los agentes en el juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(typing.NamedTuple):\n",
    "    \"\"\"Representa la transici贸n de un estado al siguiente\"\"\"\n",
    "    prev_state: int              # Estado origen de la transici贸n\n",
    "    next_state: int              # Estado destino de la transici贸n\n",
    "    action: Action               # Acci贸n que provoc贸 esta transici贸n\n",
    "    reward: typing.SupportsFloat # Recompensa obtenida"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase que define el comportamiento de un agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Agent(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def __init__(self, name: str):\n",
    "        \"\"\"Inicializa el objeto.\n",
    "        \n",
    "        :param name: El nombre del agente.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        # START\n",
    "        # END\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def decide(self, state:int) -> Action:\n",
    "        \"\"\"Decide la acci贸n a llevar a cabo dado el estado actual.\n",
    "        \n",
    "        :param state: El estado en el que se encuentra el agente.\n",
    "        :returns: La acci贸n a llevar a cabo.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        # END\n",
    "    \n",
    "    def update(self, transition: Transition):\n",
    "        \"\"\"Actualiza (si es necesario) el estado interno del agente.\n",
    "        \n",
    "        :param transition: La informaci贸n de la transici贸n efectuada.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentes de los equipos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se defin铆a en el enunciado, se permite cualquier implementaci贸n siempre y cuando:\n",
    "\n",
    "1. Herede de la clase `Agent` suministrada.\n",
    "1. El m茅todo `__init__` no admita ning煤n par谩metro adicional."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralHive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 18:58:08.278049: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 18:58:08.478415: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-01 18:58:08.558641: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-01 18:58:09.220198: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-01 18:58:09.220262: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-01 18:58:09.220266: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "from os import makedirs, cpu_count\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "saved_path = \"./Agente_NeuralHive/\"\n",
    "LisAct = list(Action)\n",
    "dictAction = {k: v for v, k in enumerate(list(Action))}\n",
    "\n",
    "# -------------------------- SETTING UP THE ENVIRONMENT --------------------------------------\n",
    "# simple game, therefore we are not using the open gym custom set up\n",
    "#---------------------------------------------------------------------------------------------\n",
    "class RPSenv():\n",
    "    def __init__ (self):\n",
    "        self.action_space = Action\t\t# integer representation of r/p/s/l/k\n",
    "        self.seed = random.seed(42) \t# make it deterministic\n",
    "        self.norm_mu = 0\t\t\t\t# center point for guassian distribution\n",
    "        self.norm_sigma = 4.0\t\t\t# sigma for std distribution \n",
    "        self.seqIndex = 0\t\t\t\t# index for pointing to the SEQ sequnce \n",
    "        self.p2Mode =  'SEQ'\t\t\t# SEQ or PRNG or LFSR\n",
    "        self.p2Count = [0, 0, 0, 0, 0] \t\t# player 2 win tie lost count\n",
    "        self.p1Count = [0, 0, 0, 0, 0]\t\t# player 1 win tie lost count\n",
    "        self.window = 10\t\t\t\t\t# window size for rate trending calc\n",
    "        self.cumWinRate, self.cumTieRate, self.cumLostRate = None, None, None\n",
    "        self.overallWinRate, self.overallTieRate, self.overallLostRate = 0, 0, 0\n",
    "        self.cumWinCount, self.cumTieCount, self.cumLostCount = None, None, None\n",
    "        self.winRateTrend, self.tieRateTrend, self.lostRateTrend = 0, 0, 0\n",
    "        self.winRateMovingAvg, self.tieRateMovingAvg, self.lostRateMovingAvg = 0, 0, 0\n",
    "        self.winRateBuf, self.tieRateBuf, self.lostRateBuf \\\n",
    "            = deque(maxlen=self.window), deque(maxlen=self.window), deque(maxlen=self.window)\n",
    "        # put all the observation state in here; shape in Keras input format\n",
    "        self.state = np.array([[ \\\n",
    "            None, None, None, \\\n",
    "            self.winRateTrend, self.tieRateTrend, self.lostRateTrend, \\\n",
    "            self.winRateMovingAvg, self.tieRateMovingAvg, self.lostRateMovingAvg \\\n",
    "            ]])  \n",
    "\n",
    "    def reset(self):\n",
    "        # reset all the state\n",
    "        self.cumWinRate, self.cumTieRate, self.cumLostRate = 0, 0, 0\n",
    "        self.cumWinCount, self.cumTieCount, self.cumLostCount = 0, 0, 0\n",
    "        self.winRateTrend, self.tieRateTrend, self.lostRateTrend = 0, 0, 0\n",
    "        self.winRateMovingAvg, self.tieRateMovingAvg, self.lostRateMovingAvg = 0, 0, 0\n",
    "        return np.array([0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "    def step(self, action, moveCount, stage):\n",
    "        # This module generate one rock-paper-sessiors move based on the Mode selected.\n",
    "        def genOneMove(self, mode, stage):\n",
    "            import random\n",
    "            def lfsr2(seed, taps, nbits):\n",
    "                sr = seed\n",
    "                while 1:\n",
    "                    xor = 1\n",
    "                    for t in taps:\n",
    "                        if (sr & (1<<(t-1))) != 0:\n",
    "                            xor ^= 1\n",
    "                    sr = (xor << nbits-1) + (sr >> 1)\n",
    "                    yield xor, sr\n",
    "                    if sr == seed:\n",
    "                        break\n",
    "            if mode == 'PRNG':\n",
    "                # change play strategy for player2 along the way \n",
    "                lowPlay  = \t{0:0, 1:1, 2:2, 3:4, 4:4} \t\t\t# key = stage number, value = r(0), p(1), s(2)\n",
    "                mean1Play = {0:1, 1:2, 2:3, 3:3, 4:0} \t\t\t# key = stage number, value = r(0), p(1), s(2)\n",
    "                mean2Play = {0:2, 1:3, 2:4, 3:0, 4:1} \t\t\t# key = stage number, value = r(0), p(1), s(2)\n",
    "                mean3Play = {0:3, 1:4, 2:0, 3:1, 4:2}\t\t\t# key = stage number, value = r(0), p(1), s(2)\n",
    "                hiPlay = \t{0:4, 1:0, 2:1, 3:2, 4:3}\t\t\t# key = stage number, value = r(0), p(1), s(2)\n",
    "                # gen a random numbe from guassian & quantize it\n",
    "                a = random.gauss(self.norm_mu, self.norm_sigma) \n",
    "                if a **2 < 1 and a > 0:  \t\t\t\t\t\t\t\t\t# bettwen 1 and 0 is the paper move\n",
    "                    play = mean2Play[stage]\n",
    "                elif a > -1 and a < 0:\t\t\t\t\t\t\t\t\t\t# else bettwen -1 and 0 is the rock move\n",
    "                    play = mean3Play[stage]\n",
    "                elif a < -1:\t\t\t\t\t\t\t\t\t\t\t\t# else lower than  -1 is the sessiors move\n",
    "                    play = lowPlay[stage] \n",
    "                elif a > 1 and a < 2:\t\t\t\t\t\t\t\t\t\t# else bettwen 1 and 2 is the lizard move\n",
    "                    play = mean1Play[stage]\n",
    "                else:\t\t\t\t\t\t\t\t\t\t\t\t\t\t# else higher than +2 is the spok move\n",
    "                    play = hiPlay[stage]\n",
    "                return LisAct[play]\n",
    "            \n",
    "            elif mode == 'SEQ':\t\t\t\t\t# simple repeating pattern as 'random generator'\n",
    "                dict = {'r':Action.ROCK, 'p':Action.PAPER, 's': Action.SCISSORS, 'l': Action.LIZARD, 'k': Action.SPOCK}\n",
    "                seqlist = 'lrpprlsskkplskrsrlpprkspslkprspksplpsrkrspslpkrrlsspsklrpsklrpllskrslpkskk'\t\t\t# the pattern sequence here\n",
    "                self.seqIndex = (self.seqIndex + 1) % len(seqlist)\n",
    "                return dict[seqlist[self.seqIndex]]\n",
    "\n",
    "        # Lo deja en una lista de numeros entre 0-5 hay que sustituirlos por los simbolos\n",
    "            elif mode == 'LFSR':\n",
    "                nbits, tapindex, seed = 12, (12,11,10,4,1), 0b11001001\n",
    "                #nbits, tapindex, seed = 8, (8,6,5,4,1), 0b11001001\n",
    "                lfsrlist = []\n",
    "                for xor, sr in lfsr2(seed, tapindex, nbits):\n",
    "                    lfsr_gen = int(bin(2**nbits+sr)[3:], base=2)\n",
    "                    lfsrlist.append(lfsr_gen % 5)\n",
    "                self.seqIndex = 0 if self.seqIndex == len(lfsrlist)-1 else self.seqIndex + 1\n",
    "                return LisAct[lfsrlist[self.seqIndex]]\n",
    "\n",
    "            else:\n",
    "                print('Error: random mode does not exist!')\n",
    "                \n",
    "        # value mode is PRNG or SEQ\n",
    "        p2Move = genOneMove(self, self.p2Mode, stage)\t\t\t# play one move from player2\n",
    "        \n",
    "        self.p2Count[dictAction[p2Move]] += 1\n",
    "        p1Move = action\n",
    "        self.p1Count[dictAction[p1Move]] += 1\n",
    "\n",
    "        # check who won, set flag and assign reward \n",
    "        win, tie, lost = 0, 0, 0\n",
    "        if MOVES_AND_REWARDS[p1Move, p2Move] == 0:\n",
    "            self.cumTieCount, tie   = self.cumTieCount  + 1, 1\n",
    "        elif MOVES_AND_REWARDS[p1Move, p2Move] == 1:\n",
    "            self.cumWinCount, win   = self.cumWinCount  + 1, 1\n",
    "        else:\n",
    "            self.cumLostCount, lost = self.cumLostCount + 1, 1\n",
    "\n",
    "        # update the running rates \n",
    "        self.cumWinRate = self.cumWinCount / moveCount\n",
    "        self.cumTieRate = self.cumTieCount / moveCount\n",
    "        self.cumLostRate = self.cumLostCount / moveCount\n",
    "        # update moving avg buffer\n",
    "        self.winRateBuf.append(self.cumWinRate) \n",
    "        self.tieRateBuf.append(self.cumTieRate)\n",
    "        self.lostRateBuf.append(self.cumLostRate)\n",
    "        # calculate trend\n",
    "        tmp = [0, 0, 0]\n",
    "        self.winRateTrend, self.tieRateTrend, self.lostRateTrend = 0, 0, 0\n",
    "        if moveCount >= self.window:\n",
    "            tmp[0] = sum(self.winRateBuf[i] for i in range(self.window)) / self.window\n",
    "            tmp[1] = sum(self.tieRateBuf[i] for i in range(self.window)) / self.window\n",
    "            tmp[2] = sum(self.lostRateBuf[i] for i in range(self.window)) / self.window\n",
    "            # win rate trend analysis\n",
    "            if self.winRateMovingAvg  < tmp[0]: \n",
    "                self.winRateTrend = 1\t\t# win rate trending up. That's good\n",
    "            else: \n",
    "                self.winRateTrend = 0\t\t# win rate trending down. That's bad\n",
    "            # tie rate trend analysis\n",
    "            if self.tieRateMovingAvg  < tmp[1]:\n",
    "                self.tieRateTrend = 1  \t\t# tie rate trending up. That's bad\n",
    "            else:\n",
    "                self.tieRateTrend = 0  \t\t# tie rate trending down.  Neutral\n",
    "            # lost rate trend analysis\n",
    "            if self.lostRateMovingAvg  < tmp[2]:\n",
    "                self.lostRateTrend = 1  \t# lst rate trending up.  That's bad\n",
    "            else:\n",
    "                self.lostRateTrend = 0  \t# lost rate trending down. That's good\n",
    "            self.winRateMovingAvg, self.tieRateMovingAvg, self.lostRateMovingAvg = tmp[0], tmp[1], tmp[2]\n",
    "        # net reward in this round\n",
    "        reward = win \t\t\t\t\t\t\t\t\t\n",
    "        # record the state and reshape it for Keras input format\n",
    "        dim = self.state.shape[1]\n",
    "        self.state = np.array([\\\n",
    "            win, tie, lost, \\\n",
    "            self.winRateTrend, self.tieRateTrend, self.lostRateTrend, \\\n",
    "            self.winRateMovingAvg, self.tieRateMovingAvg, self.lostRateMovingAvg \\\n",
    "            ]).reshape(1, dim)\n",
    "        # this game is done when it hits this goal\n",
    "        done = False \n",
    "        return self.state, reward, done, dim\n",
    "\n",
    "\n",
    "# ------------------------- class for the Double-DQN agent ---------------------------------\n",
    "# facilities utilized here:\n",
    "# 1)  Double DQN networks: one for behavior policy, one for target policy\n",
    "# 2)  Learn from  sample from pool of memories \n",
    "# 3)  Basic TD-Learning stuff:  learning rate,  gamma for discounting future rewards\n",
    "# 4)  Use of epsilon-greedy policy for controlling exploration vs exploitation\n",
    "#-------------------------------------------------------------------------------------------\n",
    "class NeuralHive(Agent):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"NeuralHive\")\n",
    "\n",
    "        if saved_path:\n",
    "            with open(f\"{saved_path}/class.pkl\", \"rb\") as saved_atrb:\n",
    "                atrb = pickle.load(saved_atrb)\n",
    "        self.env = RPSenv() if not saved_path else atrb[\"env\"]\n",
    "        self.action_space = self.env.action_space if not saved_path else atrb[\"action_space\"]\n",
    "        self.state = self.env.state if not saved_path else atrb[\"state\"]\n",
    "        # initialize the memory and auto drop when memory exceeds maxlen\n",
    "        # this controls how far out in history the \"expeience replay\" can select from\n",
    "        self.maxlen = 3000 if not saved_path else atrb[\"maxlen\"]\n",
    "        self.memory  = deque(maxlen = self.maxlen) if not saved_path else atrb[\"memory\"]\n",
    "        # future reward discount rate of the max Q of next state\n",
    "        self.gamma = 0.9 if not saved_path else atrb[\"gamma\"]\n",
    "        # epsilon denotes the fraction of time dedicated to exploration (as oppse to exploitation)\n",
    "        self.epsilon = 1.0 if not saved_path else atrb[\"epsilon\"]\n",
    "        self.epsilon_min = 0.01 if not saved_path else atrb[\"epsilon_min\"]\n",
    "        self.epsilon_decay = 0.9910 if not saved_path else atrb[\"epsilon_decay\"]\n",
    "        # model learning rate (use in backprop SGD process)\n",
    "        self.learning_rate = 0.005 if not saved_path else atrb[\"learning_rate\"]\n",
    "        # transfer learning proportion contrl between the target and action/behavioral NN\n",
    "        self.tau = .125 if not saved_path else atrb[\"tau\"]\n",
    "        # hyyperparameters for LSTM\n",
    "        self.lookback = 100 if not saved_path else atrb[\"lookback\"]\n",
    "        self.hiddenUnits = 50 if not saved_path else atrb[\"hiddenUnits\"]\n",
    "        # create two models for double-DQN implementation\n",
    "        self.model        = self.create_model() if not saved_path else load_model(f\"{saved_path}/Model_DQN_NeuralHive.h5\")\n",
    "        self.target_model = self.create_model() if not saved_path else load_model(f\"{saved_path}/TargetModel_DQN_NeuralHive.h5\")\n",
    "        # some space to collect TD target for instrumentaion\n",
    "        self.TDtarget = [] if not saved_path else atrb[\"TDtarget\"]\n",
    "        self.TDtargetdelta = [] if not saved_path else atrb[\"TDtargetdelta\"]\n",
    "        self.Qmax =[] if not saved_path else atrb[\"Qmax\"]\n",
    "        # lookback steps accumlated\n",
    "        self.step = len(self.memory)\n",
    "    \n",
    "    def save(self, dir_path):\n",
    "        makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        atrb = {\n",
    "            \"env\": self.env,\n",
    "            \"action_space\": self.action_space,\n",
    "            \"state\": self.state,\n",
    "            \"maxlen\": self.maxlen,\n",
    "            \"memory\": self.memory,\n",
    "            \"gamma\": self.gamma,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"epsilon_min\": self.epsilon_min,\n",
    "            \"epsilon_decay\": self.epsilon_decay,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"tau\": self.tau,\n",
    "            \"lookback\": self.lookback,\n",
    "            \"hiddenUnits\": self.hiddenUnits,\n",
    "            \"TDtarget\": self.TDtarget,\n",
    "            \"TDtargetdelta\": self.TDtargetdelta,\n",
    "            \"Qmax\": self.Qmax\n",
    "        }\n",
    "        with open(f\"{dir_path}/class.pkl\", \"wb\") as save_atrb:\n",
    "            pickle.dump(atrb, save_atrb)\n",
    "        \n",
    "        self.model.save(f\"{dir_path}/Model_DQN_NeuralHive.h5\", save_format=\"h5\")\n",
    "        self.target_model.save(f\"{dir_path}/TargetModel_DQN_NeuralHive.h5\", save_format=\"h5\")\n",
    "\n",
    "    def decide(self, state):\n",
    "        # AI agent take one action\n",
    "        return self.act(state, self.step)\n",
    "\n",
    "    def update(self, transition: Transition):\n",
    "        self.step += 1\n",
    "        # record the play into memory pool\n",
    "        self.remember(transition.prev_state, transition.action, transition.reward, transition.next_state, False)\n",
    "        # perform Q-learning from using |\"experience replay\": learn from random samples in memory\n",
    "        self.replay()\n",
    "        # apply tranfer learning from actions model to the target model.\n",
    "        self.target_train()\n",
    "\n",
    "    def create_model(self):\n",
    "        input_feature_dim = self.state.shape[1]\n",
    "        output_feature_dim = len(self.action_space)\n",
    "        model = Sequential()\n",
    "        #model.add(GRU(self.hiddenUnits,\\\n",
    "        model.add(LSTM(self.hiddenUnits,\\\n",
    "\t\t    return_sequences = False,\\\n",
    "\t\t    #activation = None, \\\n",
    "\t\t    #recurrent_activation = None, \\\n",
    "            input_shape = (self.lookback, input_feature_dim)))\n",
    "\t\t# let the output be the predicted target value.  NOTE: do not use activation to squash it! \n",
    "        #model.add(TimeDistributed(Dense(4)))\n",
    "        #model.add(Flatten())\n",
    "        model.add(Dense(output_feature_dim))\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def act(self, state, step):\n",
    "    \t# this is to take one action\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        # decide to take a random exploration or make a policy-based action (thru NN prediction)\n",
    "        # with a LSTM design, delay on policy prediction after at least lookback steps have accumlated\n",
    "        if np.random.random() < self.epsilon or step < self.lookback + 1:\n",
    "        \t# return a random move from action space\n",
    "            return random.choice(list(self.action_space))\n",
    "        else:\n",
    "            # return a policy move\n",
    "            state_set = np.empty((1, self.state.shape[1]))  # iniitial with 2 dims\n",
    "            for j in range(self.lookback):\n",
    "                state_tmp, _, _, _, _  = self.memory[-(j+1)]    # get the most recent state and the previous N states\n",
    "                if j == 0:\n",
    "                    state_set[0] = state_tmp                 \t# iniitalize the first record\n",
    "                else:\n",
    "                    state_set = np.concatenate((state_set, state_tmp), axis = 0)\t\t# get a consecutive set of states for LSTM prediction        \n",
    "            state_set = state_set[None, :, :]  \t\t\t\t\t# make the tensor 3 dim to align with Keras reqmt\n",
    "            #print(state_set)\n",
    "            #print(state_set.shape)\n",
    "            self.Qmax.append(max(self.model.predict(state_set, verbose=0, workers=cpu_count()*0.75)[0]))\n",
    "            return list(Action)[np.argmax(self.model.predict(state_set, verbose=0, workers=cpu_count()*0.75)[0])]\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "\t\t# store up a big pool of memory\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):  \t\t\n",
    "    \t# DeepMind \"experience replay\" method\n",
    "    \t# do the training (learning); this is DeepMind tricks of using \"Double\" model (Mnih 2015)\n",
    "    \t# the sample size from memory to learn from\n",
    "     \t#------------------------\n",
    "        # do nothing untl the memory is large enough\n",
    "        RL_batch_size = 24  # this is experience replay batch_size (not the LSTM fitting batch size)\n",
    "        if len(self.memory) < RL_batch_size: return\n",
    "        # get the samples; each sample is a sequence of consecutive states with same lookback length as LSTM definition\n",
    "        for i in range(RL_batch_size):\n",
    "            state_set     = np.empty((1, self.state.shape[1]))\n",
    "            new_state_set = np.empty((1, self.state.shape[1]))\n",
    "            if len(self.memory) <= self.lookback:\t\t\t\t\t\t\t# check if memory is large enough to retrieve the time sequence\n",
    "                return\n",
    "            else:\n",
    "                a = random.randint(0, len(self.memory) - self.lookback)\t\t# first get a random location\n",
    "            state, action, reward, new_state, done = self.memory[-(a+1)]    # retrieve a sample from memory at that loc; latest element at the end of deque\n",
    "            for j in range(self.lookback):\n",
    "                state_tmp, _, _, new_state_tmp, _  = self.memory[-(a+j+1)]  # get a consecutive set of states\n",
    "                if j == 0:\n",
    "                    state_set[0] = state_tmp\n",
    "                    new_state_set[0] = new_state_tmp\n",
    "                else:\n",
    "                    state_set = np.concatenate((state_set, state_tmp), axis = 0)\t\t# get a consecutive set of states for LSTM prediction\n",
    "                    new_state_set = np.concatenate((new_state_set, new_state_tmp), axis = 0)  \n",
    "            # do the prediction from current state\n",
    "            state_set     = state_set[None, :, :]\t\t\t\t\t\t\t# make the tensor 3 dim to align with Keras reqmt\n",
    "            new_state_set = new_state_set[None, :, :]\t\t\t\t\t\t# make the tensor 3 dim to align with Keras reqmt\n",
    "            target        = self.target_model.predict(state_set, verbose=0, workers=cpu_count()*0.75)\n",
    "            # do the Q leanring\n",
    "            if done:\n",
    "                target[0][dictAction[action]] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state_set, verbose=0, workers=cpu_count()*0.75)[0]) \n",
    "                TDtarget = reward + Q_future * self.gamma\n",
    "                self.TDtarget.append(TDtarget)\n",
    "                self.TDtargetdelta.append(TDtarget - target[0][dictAction[action]])\n",
    "                target[0][dictAction[action]] = TDtarget\t \t\t\t\n",
    "            # do one pass gradient descend using target as 'label' to train the action model\n",
    "            self.model.fit(state_set, target, batch_size = 1,  epochs = 1, verbose = 0, workers=cpu_count()*0.75)\n",
    "        \n",
    "    def target_train(self):\n",
    "    \t# transfer weights  proportionally from the action/behave model to the target model\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competici贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def competition(competitors, friendly_sets, competition_sets):\n",
    "    leaderboard = {c: 0 for c in competitors}\n",
    "    game = Game()\n",
    "    for p1, p2 in itertools.combinations(leaderboard.keys(), 2):\n",
    "        p1, p2 = p1(), p2()\n",
    "        \n",
    "        # Amistoso\n",
    "        s = 0\n",
    "        for i in range(friendly_sets):\n",
    "            a1 = p1.decide(s)\n",
    "            a2 = p2.decide(s)\n",
    "            reward = game.play(a1, a2)\n",
    "            p1.update(Transition(prev_state=s, next_state=s, action=a1, reward=reward))\n",
    "            p2.update(Transition(prev_state=s, next_state=s, action=a2, reward=-reward))\n",
    "\n",
    "         # Competici贸n\n",
    "        s = 0\n",
    "        r1 = r2 = 0\n",
    "        for i in range(competition_sets):\n",
    "            a1 = p1.decide(s)\n",
    "            a2 = p2.decide(s)\n",
    "            reward = game.play(a1, a2)\n",
    "            r1 += reward\n",
    "            r2 -= reward\n",
    "\n",
    "        # Actualizaci贸n de marcadores globales\n",
    "        if r1 > r2:\n",
    "            leaderboard[p1.__class__] += 3\n",
    "        elif r2 > r1:\n",
    "            leaderboard[p2.__class__] += 3\n",
    "        else:\n",
    "            leaderboard[p1.__class__] += 1\n",
    "            leaderboard[p2.__class__] += 1\n",
    "    \n",
    "    return leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL LEADERBOARD\n",
      "....................................................................................................\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 18:58:41.971199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-01 18:58:41.993211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-01 18:58:41.993420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-01 18:58:41.994258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 18:58:41.995778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-01 18:58:41.995936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-01 18:58:41.996029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-01 18:58:42.768631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-01 18:58:42.769178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-01 18:58:42.769191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-06-01 18:58:42.769353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-01 18:58:42.769400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5400 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0       \tNeuralHive\n"
     ]
    }
   ],
   "source": [
    "print('GLOBAL LEADERBOARD')\n",
    "\n",
    "NUM_MATCHES = 100       # N煤mero de torneos (para que sea m谩s equitativo)\n",
    "FRIENDLY_SETS = 10000   # N煤mero de partidas amistosas (para aprender)\n",
    "COMPETITION_SETS = 100  # N煤mero de partidas de competici贸n (para ganar)\n",
    "\n",
    "# Los competidores de los grupos. Se han comentado los que no cumplen las\n",
    "# condiciones que se indicaban en el enunciado.\n",
    "competitors = [\n",
    "    # AiWreck,\n",
    "    # AiGigantix, # No v谩lido\n",
    "    # Musasher,     \n",
    "    # DeepDark,\n",
    "    # KobayashiMaru,\n",
    "    # NetCoreAI,\n",
    "    NeuralHive,\n",
    "    NeuralHive,\n",
    "    # Neuronetix, # No v谩lido\n",
    "    # Rosmi,\n",
    "    # BAITbot,\n",
    "    # LizardWizardOnTheBlizzard,\n",
    "    # TechNoirAgent,\n",
    "]\n",
    "\n",
    "# Se ejecutan todos los torneos consecutivamente, guardando cada marcador\n",
    "leaderboards = []\n",
    "for i in range(COMPETITION_SETS):\n",
    "    print(f'.', end='')\n",
    "    leaderboard = competition(competitors, FRIENDLY_SETS, COMPETITION_SETS)\n",
    "    leaderboards.append(leaderboard)\n",
    "print('')\n",
    "\n",
    "# Obtenemos el marcador global haciendo la media enrte todos los torneos\n",
    "gl = {\n",
    "    c: sum(lb[c] for lb in leaderboards) / COMPETITION_SETS\n",
    "    for c in competitors\n",
    "}\n",
    "print('-' * 80)\n",
    "for k, v in sorted(gl.items(), key=lambda t: t[1], reverse=True):\n",
    "     print(f'{v:<10}\\t{k()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
